{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-27T02:19:29.339176Z",
     "iopub.status.busy": "2025-04-27T02:19:29.338616Z",
     "iopub.status.idle": "2025-04-27T02:19:29.636421Z",
     "shell.execute_reply": "2025-04-27T02:19:29.635788Z",
     "shell.execute_reply.started": "2025-04-27T02:19:29.339148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:19:31.476543Z",
     "iopub.status.busy": "2025-04-27T02:19:31.476190Z",
     "iopub.status.idle": "2025-04-27T02:19:32.048449Z",
     "shell.execute_reply": "2025-04-27T02:19:32.047830Z",
     "shell.execute_reply.started": "2025-04-27T02:19:31.476522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đọc dữ liệu\n",
    "events = pd.read_csv(\"/kaggle/input/ecommerce-dataset/events.csv\")\n",
    "items1 = pd.read_csv(\"/kaggle/input/ecommerce-dataset/item_properties_part1.csv\")\n",
    "items2 = pd.read_csv(\"/kaggle/input/ecommerce-dataset/item_properties_part2.csv\")\n",
    "categories = pd.read_csv(\"/kaggle/input/ecommerce-dataset/category_tree.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Gộp dữ liệu items\n",
    "items = pd.concat([items1, items2], ignore_index=True)\n",
    "\n",
    "# Giữ lại các event quan trọng\n",
    "events = events[events[\"event\"].isin([\"view\", \"addtocart\", \"transaction\"])]\n",
    "\n",
    "# Mã hóa user_id và item_id\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "events[\"user_id\"] = user_encoder.fit_transform(events[\"visitorid\"])\n",
    "events[\"item_id\"] = item_encoder.fit_transform(events[\"itemid\"])\n",
    "\n",
    "# Chuyển timestamp về dạng datetime\n",
    "events[\"timestamp\"] = pd.to_datetime(events[\"timestamp\"], unit=\"ms\")\n",
    "\n",
    "# Xử lý category_id\n",
    "items_category = items[items[\"property\"] == \"categoryid\"][[\"itemid\", \"value\"]]\n",
    "items_category.rename(columns={\"value\": \"categoryid\"}, inplace=True)\n",
    "items_category[\"categoryid\"] = pd.to_numeric(items_category[\"categoryid\"], errors=\"coerce\").astype(\"Int64\")\n",
    "items = items.merge(items_category, on=\"itemid\", how=\"left\")\n",
    "items_with_category = items[[\"itemid\", \"categoryid\"]].drop_duplicates()\n",
    "\n",
    "# Kết hợp category_id vào events\n",
    "df_final = events.merge(items_with_category, on=\"itemid\", how=\"left\")\n",
    "df_final.rename(columns={\"categoryid\": \"category_id\"}, inplace=True)\n",
    "\n",
    "# Xử lý missing category_id\n",
    "num_categories = df_final[\"category_id\"].nunique()\n",
    "df_final[\"category_id\"] = df_final[\"category_id\"].fillna(num_categories + 1).astype(int)\n",
    "\n",
    "# Lấy parent_category\n",
    "df_final = df_final.merge(categories, left_on=\"category_id\", right_on=\"categoryid\", how=\"left\")\n",
    "df_final.rename(columns={\"parentid\": \"parent_category\"}, inplace=True)\n",
    "df_final[\"parent_category\"] = df_final[\"parent_category\"].fillna(-1).astype(int)\n",
    "\n",
    "# Xử lý giá sản phẩm\n",
    "items_price_raw = items[items[\"value\"].str.contains(r\"n\\d+\\.\\d+\", na=False)].copy()\n",
    "def extract_price(value):\n",
    "    prices = re.findall(r\"n(\\d+\\.\\d+)\", value)\n",
    "    return float(prices[0]) if prices else None\n",
    "items_price_raw[\"price\"] = items_price_raw[\"value\"].apply(extract_price)\n",
    "items_price = items_price_raw[[\"itemid\", \"price\"]].dropna().drop_duplicates(subset=\"itemid\")\n",
    "\n",
    "# Thêm giá vào df_final\n",
    "df_final = df_final.merge(items_price, on=\"itemid\", how=\"left\")\n",
    "\n",
    "# Thêm thông tin user\n",
    "unique_users = df_final[\"user_id\"].unique()\n",
    "user_info = pd.DataFrame({\n",
    "    \"user_id\": unique_users,\n",
    "    \"age\": np.random.randint(18, 66, size=len(unique_users)),   # Tuổi 18-65\n",
    "    \"gender\": np.random.randint(0, 2, size=len(unique_users))   # 0: nữ, 1: nam\n",
    "})\n",
    "df_final = df_final.merge(user_info, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Mã hóa event thành interaction_value\n",
    "df_final[\"interaction_value\"] = df_final[\"event\"].map({\"view\": 1, \"addtocart\": 2, \"transaction\": 3}).astype(np.float32)\n",
    "\n",
    "# Chuẩn hóa tuổi\n",
    "scaler = MinMaxScaler()\n",
    "df_final[\"age\"] = scaler.fit_transform(df_final[[\"age\"]])\n",
    "\n",
    "# Xử lý missing giá bằng median\n",
    "median_price = df_final[\"price\"].median()\n",
    "df_final[\"price\"] = df_final[\"price\"].fillna(median_price)\n",
    "df_final[\"price\"] = np.log1p(df_final[\"price\"])\n",
    "df_final[\"price\"] = scaler.fit_transform(df_final[[\"price\"]])\n",
    "\n",
    "# Lưu datetime gốc để xử lý tuần hoàn\n",
    "df_final[\"datetime\"] = df_final[\"timestamp\"]\n",
    "\n",
    "# Trích xuất các feature theo thời gian\n",
    "df_final[\"hour\"] = df_final[\"datetime\"].dt.hour\n",
    "df_final[\"dayofweek\"] = df_final[\"datetime\"].dt.dayofweek\n",
    "df_final[\"month\"] = df_final[\"datetime\"].dt.month\n",
    "df_final[\"is_weekend\"] = df_final[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "\n",
    "# Encode sin/cos cho giờ và ngày\n",
    "df_final[\"hour_sin\"] = np.sin(2 * np.pi * df_final[\"hour\"] / 24)\n",
    "df_final[\"hour_cos\"] = np.cos(2 * np.pi * df_final[\"hour\"] / 24)\n",
    "df_final[\"dayofweek_sin\"] = np.sin(2 * np.pi * df_final[\"dayofweek\"] / 7)\n",
    "df_final[\"dayofweek_cos\"] = np.cos(2 * np.pi * df_final[\"dayofweek\"] / 7)\n",
    "\n",
    "# Tính recency và session\n",
    "df_final = df_final.sort_values(by=[\"user_id\", \"timestamp\"])\n",
    "df_final[\"timestamp_numeric\"] = df_final[\"timestamp\"].astype(np.int64) / 1e9\n",
    "df_final[\"time_diff\"] = df_final.groupby(\"user_id\")[\"timestamp_numeric\"].diff().fillna(0)\n",
    "df_final[\"new_session\"] = (df_final[\"time_diff\"] > 1800).astype(int)\n",
    "df_final[\"session_id\"] = df_final.groupby(\"user_id\")[\"new_session\"].cumsum()\n",
    "\n",
    "# Chuẩn hóa timestamp liên tục\n",
    "df_final[\"timestamp_norm\"] = scaler.fit_transform(df_final[[\"timestamp_numeric\"]])\n",
    "\n",
    "# Lọc cột cuối cùng\n",
    "df_final = df_final[[\n",
    "    \"user_id\", \"item_id\", \"category_id\", \"parent_category\", \"price\", \"age\", \"gender\",\n",
    "    \"interaction_value\", \"hour\", \"dayofweek\", \"is_weekend\",\n",
    "    \"hour_sin\", \"hour_cos\", \"dayofweek_sin\", \"dayofweek_cos\",\n",
    "    \"time_diff\", \"session_id\", \"timestamp_norm\"\n",
    "]]\n",
    "\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Count number of sessions per user\n",
    "session_counts = df_final.groupby(\"user_id\")[\"session_id\"].nunique().reset_index()\n",
    "session_counts.columns = [\"user_id\", \"num_sessions\"]\n",
    "\n",
    "# Count how many users have each number of sessions\n",
    "session_count_values = session_counts[\"num_sessions\"].value_counts().reset_index()\n",
    "session_count_values.columns = [\"num_sessions\", \"user_count\"]\n",
    "\n",
    "# Create a complete range from 1 to 30\n",
    "full_range = pd.DataFrame({'num_sessions': range(1, 31)})\n",
    "\n",
    "# Merge with our actual data to fill in zeros where needed\n",
    "session_count_values = full_range.merge(session_count_values, \n",
    "                                       on='num_sessions', \n",
    "                                       how='left').fillna(0)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(18, 6))  # Increase figure width for better visibility\n",
    "ax = sns.barplot(x=\"num_sessions\", \n",
    "                 y=\"user_count\", \n",
    "                 data=session_count_values,\n",
    "                 color=\"lightcoral\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Phân bố số lượng sessions theo người dùng\", fontsize=14, pad=20)\n",
    "plt.xlabel(\"Số sessions\", fontsize=12, labelpad=10)\n",
    "plt.ylabel(\"Số người dùng\", fontsize=12, labelpad=10)\n",
    "\n",
    "# Set x-axis to show all integers from 1 to 30\n",
    "plt.xticks(range(30), range(1, 31))\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add grid lines\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Remove spines for cleaner look\n",
    "sns.despine()\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > 0:  # Only label bars with height > 0\n",
    "        ax.annotate(f\"{int(p.get_height())}\", \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha='center', va='center', \n",
    "                   xytext=(0, 5), \n",
    "                   textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Trực quan hóa phân bố của interaction_value\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=\"interaction_value\", data=df_final, palette=\"Set2\")\n",
    "plt.title(\"Phân bố interaction_value\", fontsize=14)\n",
    "plt.xlabel(\"Interaction Value\", fontsize=12)\n",
    "plt.ylabel(\"Số lượng\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_final[\"price\"], kde=True, color=\"skyblue\", bins=30)\n",
    "plt.title(\"Phân bố giá sản phẩm\", fontsize=14)\n",
    "plt.xlabel(\"Giá (log1p)\", fontsize=12)\n",
    "plt.ylabel(\"Số lượng\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_final[\"time_diff\"], kde=True, color=\"orange\", bins=50)\n",
    "plt.title(\"Phân bố thời gian giữa các hành động (Recency)\", fontsize=14)\n",
    "plt.xlabel(\"Thời gian giữa các hành động (giây)\", fontsize=12)\n",
    "plt.ylabel(\"Số lượng\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=\"dayofweek\", data=df_final, palette=\"Blues\")\n",
    "plt.title(\"Phân bố theo ngày trong tuần\", fontsize=14)\n",
    "plt.xlabel(\"Ngày trong tuần\", fontsize=12)\n",
    "plt.ylabel(\"Số lượng\", fontsize=12)\n",
    "plt.xticks(ticks=range(7), labels=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=\"hour\", data=df_final, palette=\"coolwarm\")\n",
    "plt.title(\"Phân bố theo giờ trong ngày\", fontsize=14)\n",
    "plt.xlabel(\"Giờ trong ngày\", fontsize=12)\n",
    "plt.ylabel(\"Số lượng\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightFM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 80% train-test\n",
    "train, test = train_test_split(df_final, test_size=0.2, random_state=42)\n",
    "# 70% train và 10% validation\n",
    "train, valid = train_test_split(train, test_size=0.125, random_state=42)  # 0.125 * 80% = 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:19:43.303639Z",
     "iopub.status.busy": "2025-04-27T02:19:43.303254Z",
     "iopub.status.idle": "2025-04-27T02:19:43.308252Z",
     "shell.execute_reply": "2025-04-27T02:19:43.307613Z",
     "shell.execute_reply.started": "2025-04-27T02:19:43.303618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"user_id\": \"int32\",\n",
    "    \"item_id\": \"int32\",\n",
    "    \"category_id\": \"int32\",\n",
    "    \"parent_category\": \"int32\",\n",
    "    \"price\": \"float32\",\n",
    "    \"age\": \"float32\",\n",
    "    \"gender\": \"float32\",\n",
    "    \"interaction_value\": \"float32\",\n",
    "    \n",
    "    # Các thuộc tính mới\n",
    "    \"hour\": \"float32\",\n",
    "    \"dayofweek\": \"float32\",\n",
    "    \"is_weekend\": \"float32\",\n",
    "    \"hour_sin\": \"float32\",\n",
    "    \"hour_cos\": \"float32\",\n",
    "    \"dayofweek_sin\": \"float32\",\n",
    "    \"dayofweek_cos\": \"float32\",\n",
    "    \"time_diff\": \"float32\",\n",
    "    \"session_id\": \"int32\",\n",
    "    \"timestamp_norm\": \"float32\"\n",
    "}\n",
    "# for col, dtype in dtypes.items():\n",
    "#     df_final[col] = df_final[col].astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train.to_csv(\"train.csv\", index=False)\n",
    "# valid.to_csv(\"valid.csv\", index=False)\n",
    "# test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:19:45.803531Z",
     "iopub.status.busy": "2025-04-27T02:19:45.802820Z",
     "iopub.status.idle": "2025-04-27T02:20:00.278109Z",
     "shell.execute_reply": "2025-04-27T02:20:00.277329Z",
     "shell.execute_reply.started": "2025-04-27T02:19:45.803506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Datasets/NEW_retail/train.csv\", dtype=dtypes)\n",
    "valid = pd.read_csv(\"Datasets/NEW_retail/valid.csv\", dtype=dtypes)\n",
    "test = pd.read_csv(\"Datasets/NEW_retail/test.csv\", dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:20:00.279594Z",
     "iopub.status.busy": "2025-04-27T02:20:00.279322Z",
     "iopub.status.idle": "2025-04-27T02:20:12.343750Z",
     "shell.execute_reply": "2025-04-27T02:20:12.342963Z",
     "shell.execute_reply.started": "2025-04-27T02:20:00.279567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\__internal__\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\__internal__\\models\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone_and_build_model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_place_subclassed_model_state_restoration\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\applications\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\applications\\convnext.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imagenet_utils\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sequential\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\engine\\sequential.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers \u001b[38;5;28;01mas\u001b[39;00m layer_module\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_layer\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\engine\\functional.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_spec\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_utils\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\engine\\training.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v1\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pickle_utils\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_api\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m legacy_sm_saving_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m saved_model_load\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_context\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m saved_model_save\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_option_scope\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load_context.py:68\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether under a load context.\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_context\u001b[38;5;241m.\u001b[39min_load_context()\n\u001b[1;32m---> 68\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_load_context_function\u001b[49m(in_load_context)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(f\"Train Shape: {train.shape}\")\n",
    "print(f\"Validation Shape: {valid.shape}\")\n",
    "print(f\"Test Shape: {test.shape}\")\n",
    "\n",
    "print(\"Example data:\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T07:14:52.133372Z",
     "iopub.status.busy": "2025-04-26T07:14:52.132531Z",
     "iopub.status.idle": "2025-04-26T07:14:52.163100Z",
     "shell.execute_reply": "2025-04-26T07:14:52.162100Z",
     "shell.execute_reply.started": "2025-04-26T07:14:52.133343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train[\"interaction_value\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:20:12.345066Z",
     "iopub.status.busy": "2025-04-27T02:20:12.344634Z",
     "iopub.status.idle": "2025-04-27T02:20:25.587018Z",
     "shell.execute_reply": "2025-04-27T02:20:25.586300Z",
     "shell.execute_reply.started": "2025-04-27T02:20:12.345047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:20:25.589433Z",
     "iopub.status.busy": "2025-04-27T02:20:25.589193Z",
     "iopub.status.idle": "2025-04-27T02:26:48.888258Z",
     "shell.execute_reply": "2025-04-27T02:26:48.887560Z",
     "shell.execute_reply.started": "2025-04-27T02:20:25.589412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import csr_matrix  \n",
    "\n",
    "num_threads = min(os.cpu_count(), 8)\n",
    "\n",
    "# Chuẩn bị dataset với user_id và item_id\n",
    "all_user_ids = np.unique(np.concatenate((train[\"user_id\"].unique(),\n",
    "                                         valid[\"user_id\"].unique(),\n",
    "                                         test[\"user_id\"].unique())))\n",
    "all_item_ids = np.unique(np.concatenate((train[\"item_id\"].unique(),\n",
    "                                         valid[\"item_id\"].unique(),\n",
    "                                         test[\"item_id\"].unique())))\n",
    "dataset = Dataset()\n",
    "dataset.fit(users=all_user_ids, items=all_item_ids)\n",
    "\n",
    "# Xây dựng interactions matrix\n",
    "(interactions_train, _) = dataset.build_interactions(zip(train[\"user_id\"], train[\"item_id\"], train[\"interaction_value\"]))\n",
    "(interactions_valid, _) = dataset.build_interactions(zip(valid[\"user_id\"], valid[\"item_id\"], valid[\"interaction_value\"]))\n",
    "(interactions_test, _) = dataset.build_interactions(zip(test[\"user_id\"], test[\"item_id\"], test[\"interaction_value\"]))\n",
    "\n",
    "# Chuyển interactions sang dạng sparse matrix (CSR) - Ma trận thưa\n",
    "interactions_train_csr = csr_matrix(interactions_train)\n",
    "interactions_valid_csr = csr_matrix(interactions_valid)\n",
    "interactions_test_csr = csr_matrix(interactions_test)\n",
    "\n",
    "embedding_dim = 128  \n",
    "model_LightFM = LightFM(loss=\"warp\", no_components=embedding_dim)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
    "    model_LightFM.fit_partial(interactions_train_csr, epochs=1, num_threads=num_threads)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} finished!\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tạo ma trận R (4 người dùng x 5 sản phẩm)\n",
    "R = np.array([\n",
    "    [5, 3, 0, 1, 0],\n",
    "    [4, 0, 0, 1, 0],\n",
    "    [1, 1, 0, 5, 4],\n",
    "    [0, 0, 5, 4, 0]\n",
    "])\n",
    "\n",
    "# Áp dụng SVD\n",
    "U, s, VT = np.linalg.svd(R, full_matrices=False)\n",
    "Sigma = np.diag(s)\n",
    "\n",
    "# Giảm số chiều xuống k=2 (ví dụ: 2 đặc trưng tiềm ẩn)\n",
    "k = 2\n",
    "U_k = U[:, :k]\n",
    "Sigma_k = Sigma[:k, :k]\n",
    "VT_k = VT[:k, :]\n",
    "\n",
    "# Thiết lập đồ thị\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "sns.heatmap(R, cmap=\"Blues\", cbar=True, annot=True, ax=axs[0])\n",
    "axs[0].set_title(\"Ma trận R (4x5)\")\n",
    "axs[0].set_xlabel(\"Sản phẩm\")\n",
    "axs[0].set_ylabel(\"Người dùng\")\n",
    "\n",
    "sns.heatmap(U_k, cmap=\"Greens\", cbar=True, annot=True, ax=axs[1])\n",
    "axs[1].set_title(\"U (4x2)\")\n",
    "axs[1].set_xlabel(\"Đặc trưng ẩn\")\n",
    "axs[1].set_ylabel(\"Người dùng\")\n",
    "\n",
    "sns.heatmap(Sigma_k, cmap=\"Oranges\", cbar=True, annot=True, ax=axs[2])\n",
    "axs[2].set_title(\"Σ (2x2)\")\n",
    "axs[2].set_xlabel(\"Đặc trưng ẩn\")\n",
    "axs[2].set_ylabel(\"Đặc trưng ẩn\")\n",
    "\n",
    "sns.heatmap(VT_k, cmap=\"Purples\", cbar=True, annot=True, ax=axs[3])\n",
    "axs[3].set_title(\"Vᵀ (2x5)\")\n",
    "axs[3].set_xlabel(\"Sản phẩm\")\n",
    "axs[3].set_ylabel(\"Đặc trưng ẩn\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Phân rã ma trận R ≈ UΣVᵀ bằng SVD (k=2)\", y=1.05, fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from lightfm.evaluation import precision_at_k, recall_at_k, auc_score\n",
    "\n",
    "precision = precision_at_k(model_LightFM, interactions_test_csr, k=5, num_threads=num_threads).mean()\n",
    "recall = recall_at_k(model_LightFM, interactions_test_csr, k=5, num_threads=num_threads).mean()\n",
    "auc = auc_score(model_LightFM, interactions_test_csr, num_threads=num_threads).mean()\n",
    "\n",
    "print(f\"Precision@5: {precision:.4f}\")\n",
    "print(f\"Recall@5: {recall:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:48.889170Z",
     "iopub.status.busy": "2025-04-27T02:26:48.888887Z",
     "iopub.status.idle": "2025-04-27T02:26:48.894111Z",
     "shell.execute_reply": "2025-04-27T02:26:48.893525Z",
     "shell.execute_reply.started": "2025-04-27T02:26:48.889112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Trích xuất embeddings\n",
    "user_embeddings = model_LightFM.user_embeddings\n",
    "item_embeddings = model_LightFM.item_embeddings\n",
    "print(\"Shape of user embedding: \",user_embeddings.shape)\n",
    "print(\"Shape of item embedding: \",item_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lưu embeddings ra file .npy\n",
    "np.save(\"user_embeddings.npy\", user_embeddings)\n",
    "np.save(\"item_embeddings.npy\", item_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# user_embeddings = np.load(\"user_embeddings.npy\")\n",
    "# item_embeddings = np.load(\"item_embeddings.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:48.894929Z",
     "iopub.status.busy": "2025-04-27T02:26:48.894770Z",
     "iopub.status.idle": "2025-04-27T02:26:48.910657Z",
     "shell.execute_reply": "2025-04-27T02:26:48.910156Z",
     "shell.execute_reply.started": "2025-04-27T02:26:48.894916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_users, embedding_dim = user_embeddings.shape\n",
    "num_products = item_embeddings.shape[0]\n",
    "\n",
    "print(\"Number of users = \", num_users)\n",
    "print(\"Number of products = \", num_products)\n",
    "print(\"Demension embedding = \", embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:48.911727Z",
     "iopub.status.busy": "2025-04-27T02:26:48.911476Z",
     "iopub.status.idle": "2025-04-27T02:26:48.948979Z",
     "shell.execute_reply": "2025-04-27T02:26:48.948334Z",
     "shell.execute_reply.started": "2025-04-27T02:26:48.911707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kiểm tra số lượng danh mục sản phẩm\n",
    "num_categories = train[\"category_id\"].nunique()\n",
    "print(\"Number of categories =\", num_categories)\n",
    "\n",
    "# Kiểm tra số lượng unique parent_category\n",
    "num_parent_categories = train[\"parent_category\"].nunique()\n",
    "print(\"Number of parent categories =\", num_parent_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN + LightFM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Build Model LightFM + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:48.949943Z",
     "iopub.status.busy": "2025-04-27T02:26:48.949747Z",
     "iopub.status.idle": "2025-04-27T02:26:49.361671Z",
     "shell.execute_reply": "2025-04-27T02:26:49.361071Z",
     "shell.execute_reply.started": "2025-04-27T02:26:48.949919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "y_labels = train[\"interaction_value\"].values - 1  # trừ 1 để về dạng [0, 1, 2]\n",
    "class_weights_np = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_labels),\n",
    "    y=y_labels\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights_np))\n",
    "print(\"Class Weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:49.363218Z",
     "iopub.status.busy": "2025-04-27T02:26:49.362457Z",
     "iopub.status.idle": "2025-04-27T02:26:49.367442Z",
     "shell.execute_reply": "2025-04-27T02:26:49.366780Z",
     "shell.execute_reply.started": "2025-04-27T02:26:49.363197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def focal_loss_with_class_weights(class_weights, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        focal = tf.pow(1 - y_pred, gamma)\n",
    "        class_weights_tensor = tf.constant(class_weights, dtype=tf.float32)\n",
    "        weighted_focal = class_weights_tensor * focal * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(weighted_focal, axis=-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:49.369745Z",
     "iopub.status.busy": "2025-04-27T02:26:49.369463Z",
     "iopub.status.idle": "2025-04-27T02:26:49.383113Z",
     "shell.execute_reply": "2025-04-27T02:26:49.382350Z",
     "shell.execute_reply.started": "2025-04-27T02:26:49.369727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metadata_config = {\n",
    "    \"user_input\":          {\"shape\": (1,), \"dtype\": 'int32', \"embedding\": True, \"dim\": num_users, \"weights\": user_embeddings},\n",
    "    \"product_input\":       {\"shape\": (1,), \"dtype\": 'int32', \"embedding\": True, \"dim\": num_products, \"weights\": item_embeddings},\n",
    "    \"category_input\":      {\"shape\": (1,), \"dtype\": 'int32', \"embedding\": True, \"dim\": num_categories},\n",
    "    \"parent_category_input\": {\"shape\": (1,), \"dtype\": 'int32', \"embedding\": True, \"dim\": num_parent_categories},\n",
    "    \"timestamp_input\":     {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \"price_input\":         {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \"age_input\":           {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \"gender_input\":        {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "\n",
    "    \"hour_input\":          {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \"dayofweek_input\":     {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \"is_weekend_input\":    {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \n",
    "    \"hour_sin_input\":      {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \"hour_cos_input\":      {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \"dayofweek_sin_input\": {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \"dayofweek_cos_input\": {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \n",
    "    \"time_diff_input\":     {\"shape\": (1,), \"dtype\": 'float32', \"dense\": True},\n",
    "    \"session_id_input\":    {\"shape\": (1,), \"dtype\": 'int32', \"dense\": True}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:49.383954Z",
     "iopub.status.busy": "2025-04-27T02:26:49.383755Z",
     "iopub.status.idle": "2025-04-27T02:26:53.479241Z",
     "shell.execute_reply": "2025-04-27T02:26:53.478493Z",
     "shell.execute_reply.started": "2025-04-27T02:26:49.383938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Embedding, Flatten, Dense, Dropout, Concatenate,\n",
    "                                     Multiply, BatchNormalization, Reshape, GaussianNoise, Activation, Lambda, Layer, Conv1D, GlobalMaxPooling1D, LSTM)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(trainable_embedding=False):\n",
    "    inputs = {}\n",
    "    embeddings = {}\n",
    "    dense_features = []\n",
    "\n",
    "    # Print input shapes for debugging\n",
    "    print(f\"User embedding matrix shape: {user_embeddings.shape}\")\n",
    "    print(f\"Product embedding matrix shape: {item_embeddings.shape}\")\n",
    "    \n",
    "    for name, cfg in metadata_config.items():\n",
    "        inp = Input(shape=cfg[\"shape\"], dtype=cfg[\"dtype\"], name=name)\n",
    "        inputs[name] = inp\n",
    "\n",
    "        if cfg.get(\"embedding\"):\n",
    "            # Special handling for user and product embeddings\n",
    "            if name == \"user_input\":\n",
    "                emb_weights = [user_embeddings]\n",
    "                emb_dim = user_embeddings.shape[1]\n",
    "            elif name == \"product_input\":\n",
    "                emb_weights = [item_embeddings]\n",
    "                emb_dim = item_embeddings.shape[1]\n",
    "            else:\n",
    "                emb_weights = None\n",
    "                emb_dim = embedding_dim\n",
    "                \n",
    "            emb_layer = Embedding(\n",
    "                input_dim=cfg[\"dim\"], \n",
    "                output_dim=emb_dim,\n",
    "                weights=emb_weights,\n",
    "                trainable=trainable_embedding\n",
    "            )(inp)\n",
    "            \n",
    "            emb_layer = GaussianNoise(0.01)(emb_layer)\n",
    "            emb_layer = Flatten()(emb_layer)\n",
    "            embeddings[name] = emb_layer\n",
    "\n",
    "        elif cfg.get(\"dense\"):\n",
    "            dense = Dense(16, activation='relu')(inp)\n",
    "            dense = BatchNormalization()(dense)\n",
    "            dense = Dropout(0.2)(dense)\n",
    "            dense_features.append(dense)\n",
    "            \n",
    "    # === Conv1D Layer ===\n",
    "    user_emb = embeddings[\"user_input\"]\n",
    "    product_emb = embeddings[\"product_input\"]\n",
    "    \n",
    "    # Reshape embeddings to have an additional dimension for Conv1D (batch_size, sequence_length, embedding_dim)\n",
    "    user_emb_reshaped = Reshape((-1, 1))(user_emb)  # Reshape to (batch_size, 1, embedding_dim)\n",
    "    product_emb_reshaped = Reshape((-1, 1))(product_emb)  # Reshape to (batch_size, 1, embedding_dim)\n",
    "    \n",
    "    # Apply Conv1D\n",
    "    user_emb_conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(user_emb_reshaped)\n",
    "    user_emb_conv = GlobalMaxPooling1D()(user_emb_conv)  # Dùng GlobalMaxPooling1D để giảm chiều\n",
    "\n",
    "    product_emb_conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(product_emb_reshaped)\n",
    "    product_emb_conv = GlobalMaxPooling1D()(product_emb_conv)\n",
    "\n",
    "    # === Attention Mechanism ===\n",
    "    user_emb = embeddings[\"user_input\"]\n",
    "    product_emb = embeddings[\"product_input\"]\n",
    "    \n",
    "    # Reshape for attention (batch_size, 1, embedding_dim)\n",
    "    expand_dims = Lambda(lambda x: tf.expand_dims(x, axis=1))\n",
    "    user_vector = expand_dims(user_emb)\n",
    "    product_vector = expand_dims(product_emb)\n",
    "    \n",
    "    # Verify shapes\n",
    "    print(f\"User vector shape: {user_vector.shape}\")\n",
    "    print(f\"Product vector shape: {product_vector.shape}\")\n",
    "\n",
    "    # MultiHeadAttention - using 4 heads with key_dim=32 (128/4)\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=4, \n",
    "        key_dim=32  # 128-dim embeddings divided by 4 heads\n",
    "    )(user_vector, product_vector)\n",
    "    \n",
    "    attention_output = Flatten()(attention_output)\n",
    "    attention_output = BatchNormalization()(attention_output)\n",
    "\n",
    "    # Element-wise product of user and product embeddings\n",
    "    user_product_interaction = Multiply()([user_emb, product_emb])\n",
    "\n",
    "    # Merge all features\n",
    "    merged = Concatenate()([\n",
    "        user_emb, \n",
    "        product_emb, \n",
    "        user_product_interaction,\n",
    "        embeddings.get(\"category_input\", []),\n",
    "        embeddings.get(\"parent_category_input\", []),\n",
    "        attention_output,\n",
    "        *dense_features\n",
    "    ])\n",
    "    \n",
    "    print(f\"Merged features shape: {merged.shape}\")\n",
    "\n",
    "    # === Dense Layers ===\n",
    "    x = Dense(512, kernel_regularizer=l2(1e-4))(merged)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(256, kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(64, kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    print(\"Before output layer:\", x.shape)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=list(inputs.values()), outputs=output)\n",
    "    \n",
    "    # model.compile(\n",
    "    #     optimizer=Adam(learning_rate=1e-3),\n",
    "    #     loss=focal_loss_with_class_weights(class_weights=class_weights_np, gamma=2.0),\n",
    "    #     metrics=['accuracy']\n",
    "    # )\n",
    "    \n",
    "    # for layer in model.layers:\n",
    "    #     print(f\"{layer.name}: {layer.output.shape}\")\n",
    "\n",
    "    return model\n",
    "    \n",
    "# Build the model\n",
    "model = build_model(trainable_embedding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-26T07:30:12.114869Z",
     "iopub.status.busy": "2025-04-26T07:30:12.114564Z",
     "iopub.status.idle": "2025-04-26T07:30:12.189929Z",
     "shell.execute_reply": "2025-04-26T07:30:12.189130Z",
     "shell.execute_reply.started": "2025-04-26T07:30:12.114845Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:53.480782Z",
     "iopub.status.busy": "2025-04-27T02:26:53.480092Z",
     "iopub.status.idle": "2025-04-27T02:26:53.488300Z",
     "shell.execute_reply": "2025-04-27T02:26:53.487536Z",
     "shell.execute_reply.started": "2025-04-27T02:26:53.480763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=15,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='model_frozen_embeddings.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:53.489287Z",
     "iopub.status.busy": "2025-04-27T02:26:53.489007Z",
     "iopub.status.idle": "2025-04-27T02:26:53.563440Z",
     "shell.execute_reply": "2025-04-27T02:26:53.562659Z",
     "shell.execute_reply.started": "2025-04-27T02:26:53.489265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Prepare data\n",
    "X_test = [\n",
    "    test[\"user_id\"].values,\n",
    "    test[\"item_id\"].values,\n",
    "    test[\"category_id\"].values,\n",
    "    test[\"parent_category\"].values,\n",
    "    test[\"timestamp_norm\"].values,\n",
    "    test[\"price\"].values,\n",
    "    test[\"age\"].values,\n",
    "    test[\"gender\"].values,\n",
    "    \n",
    "    test[\"hour\"].values,\n",
    "    test[\"dayofweek\"].values,\n",
    "    test[\"is_weekend\"].values,\n",
    "    \n",
    "    test[\"hour_sin\"].values,\n",
    "    test[\"hour_cos\"].values,\n",
    "    test[\"dayofweek_sin\"].values,\n",
    "    test[\"dayofweek_cos\"].values,\n",
    "    \n",
    "    test[\"time_diff\"].values,\n",
    "    test[\"session_id\"].values\n",
    "]\n",
    "\n",
    "X_valid = [\n",
    "    valid[\"user_id\"].values,\n",
    "    valid[\"item_id\"].values,\n",
    "    valid[\"category_id\"].values,\n",
    "    valid[\"parent_category\"].values,\n",
    "    valid[\"timestamp_norm\"].values,\n",
    "    valid[\"price\"].values,\n",
    "    valid[\"age\"].values,\n",
    "    valid[\"gender\"].values,\n",
    "    \n",
    "    valid[\"hour\"].values,\n",
    "    valid[\"dayofweek\"].values,\n",
    "    valid[\"is_weekend\"].values,\n",
    "    \n",
    "    valid[\"hour_sin\"].values,\n",
    "    valid[\"hour_cos\"].values,\n",
    "    valid[\"dayofweek_sin\"].values,\n",
    "    valid[\"dayofweek_cos\"].values,\n",
    "    \n",
    "    valid[\"time_diff\"].values,\n",
    "    valid[\"session_id\"].values\n",
    "]\n",
    "\n",
    "X_train = [\n",
    "    train[\"user_id\"].values,\n",
    "    train[\"item_id\"].values,\n",
    "    train[\"category_id\"].values,\n",
    "    train[\"parent_category\"].values,\n",
    "    train[\"timestamp_norm\"].values,\n",
    "    train[\"price\"].values,\n",
    "    train[\"age\"].values,\n",
    "    train[\"gender\"].values,\n",
    "    \n",
    "    train[\"hour\"].values,\n",
    "    train[\"dayofweek\"].values,\n",
    "    train[\"is_weekend\"].values,\n",
    "    \n",
    "    train[\"hour_sin\"].values,\n",
    "    train[\"hour_cos\"].values,\n",
    "    train[\"dayofweek_sin\"].values,\n",
    "    train[\"dayofweek_cos\"].values,\n",
    "    \n",
    "    train[\"time_diff\"].values,\n",
    "    train[\"session_id\"].values\n",
    "]\n",
    "\n",
    "y_train = to_categorical(y_labels, num_classes=3)\n",
    "y_valid = to_categorical(valid[\"interaction_value\"].values - 1, num_classes=3)\n",
    "y_test = to_categorical(test[\"interaction_value\"].values - 1, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:26:53.564687Z",
     "iopub.status.busy": "2025-04-27T02:26:53.564411Z",
     "iopub.status.idle": "2025-04-27T02:29:34.507446Z",
     "shell.execute_reply": "2025-04-27T02:29:34.506781Z",
     "shell.execute_reply.started": "2025-04-27T02:26:53.564666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import AUC,Precision, Recall\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss=focal_loss_with_class_weights(class_weights=class_weights_np, gamma=2.0),\n",
    "    metrics=['accuracy', AUC(), Precision(), Recall()]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    callbacks= callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:29:34.508471Z",
     "iopub.status.busy": "2025-04-27T02:29:34.508187Z",
     "iopub.status.idle": "2025-04-27T02:30:25.291760Z",
     "shell.execute_reply": "2025-04-27T02:30:25.291209Z",
     "shell.execute_reply.started": "2025-04-27T02:29:34.508451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate mô hình và in kết quả\n",
    "test_loss, test_accuracy, test_auc, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test AUC: {test_auc:.4f}, \" \\\n",
    "      f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:30:25.292843Z",
     "iopub.status.busy": "2025-04-27T02:30:25.292573Z",
     "iopub.status.idle": "2025-04-27T02:31:31.834639Z",
     "shell.execute_reply": "2025-04-27T02:31:31.833960Z",
     "shell.execute_reply.started": "2025-04-27T02:30:25.292819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# In ra confusion matrix và classification report\n",
    "print(\"classification_report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Vẽ confusion matrix bằng seaborn\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T08:03:59.049441Z",
     "iopub.status.busy": "2025-04-26T08:03:59.048926Z",
     "iopub.status.idle": "2025-04-26T08:03:59.054319Z",
     "shell.execute_reply": "2025-04-26T08:03:59.053695Z",
     "shell.execute_reply.started": "2025-04-26T08:03:59.049420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "early_stopping_ft = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10, \n",
    "    restore_best_weights=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "reduce_lr_ft = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5, \n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint_ft = ModelCheckpoint(\n",
    "    filepath='best_model_finetune.keras',  \n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_finetune = [early_stopping_ft, reduce_lr_ft, model_checkpoint_ft]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T08:12:26.199399Z",
     "iopub.status.busy": "2025-04-26T08:12:26.198507Z",
     "iopub.status.idle": "2025-04-26T08:13:00.489361Z",
     "shell.execute_reply": "2025-04-26T08:13:00.488293Z",
     "shell.execute_reply.started": "2025-04-26T08:12:26.199374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune embedding\n",
    "for layer in model.layers:\n",
    "    if 'user_emb' in layer.name or 'product_emb' in layer.name:\n",
    "        layer.trainable = True\n",
    "\n",
    "# Re-compile với learning rate nhỏ hơn\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "            loss=focal_loss_with_class_weights(class_weights=class_weights_np, gamma=2.0),\n",
    "              metrics=['accuracy', AUC(), Precision(), Recall()])\n",
    "\n",
    "# Fine-tuning\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=1,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    callbacks= callbacks_finetune\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T08:09:03.830372Z",
     "iopub.status.busy": "2025-04-26T08:09:03.830103Z",
     "iopub.status.idle": "2025-04-26T08:09:51.728696Z",
     "shell.execute_reply": "2025-04-26T08:09:51.727800Z",
     "shell.execute_reply.started": "2025-04-26T08:09:03.830353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate mô hình và in kết quả\n",
    "test_loss, test_accuracy, test_auc, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test AUC: {test_auc:.4f}, \" \\\n",
    "      f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Giả sử model đã được huấn luyện và có dữ liệu đầu vào X_test, y_test\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# In ra confusion matrix và classification report\n",
    "print(\"classification_report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Vẽ confusion matrix bằng seaborn\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "val_loss, val_accuracy, val_auc, val_precision, val_recall = model.evaluate(X_valid, y_valid)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation AUC: {val_auc:.4f}, Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test AUC: {test_auc:.4f}, \" \\\n",
    "      f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Evaluate on train set\n",
    "train_loss, train_accuracy, train_auc, train_precision, train_recall = model.evaluate(X_train, y_train)\n",
    "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train AUC: {train_auc:.4f}, \" \\\n",
    "      f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Dự đoán kết quả từ mô hình\n",
    "y_pred = model.predict(X_test)  # x_test là input test\n",
    "\n",
    "# Chuyển one-hot sang label (class index)\n",
    "y_true_cls = np.argmax(y_test, axis=1)\n",
    "y_pred_cls = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# In báo cáo phân loại và độ chính xác\n",
    "print(classification_report(y_true_cls, y_pred_cls))\n",
    "print(\"Accuracy:\", accuracy_score(y_true_cls, y_pred_cls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold start problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:31:52.395720Z",
     "iopub.status.busy": "2025-04-27T02:31:52.395447Z",
     "iopub.status.idle": "2025-04-27T02:31:52.399823Z",
     "shell.execute_reply": "2025-04-27T02:31:52.399054Z",
     "shell.execute_reply.started": "2025-04-27T02:31:52.395699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:31:54.977608Z",
     "iopub.status.busy": "2025-04-27T02:31:54.977334Z",
     "iopub.status.idle": "2025-04-27T02:31:56.124624Z",
     "shell.execute_reply": "2025-04-27T02:31:56.124066Z",
     "shell.execute_reply.started": "2025-04-27T02:31:54.977581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tách người dùng và sản phẩm cold-start\n",
    "train_users, train_items = set(train[\"user_id\"]), set(train[\"item_id\"])\n",
    "test_users, test_items = set(test[\"user_id\"]), set(test[\"item_id\"])\n",
    "\n",
    "cold_start_users = test_users - train_users\n",
    "cold_start_items = test_items - train_items\n",
    "\n",
    "mask_cold_user = test[\"user_id\"].isin(cold_start_users)\n",
    "mask_cold_item = test[\"item_id\"].isin(cold_start_items)\n",
    "\n",
    "test_cold_user, test_cold_item = test[mask_cold_user], test[mask_cold_item]\n",
    "test_non_cold_user, test_non_cold_item = test[~mask_cold_user], test[~mask_cold_item]\n",
    "\n",
    "y_test_cold_user, y_test_cold_item = y_test[mask_cold_user.values], y_test[mask_cold_item.values]\n",
    "y_test_non_cold_user, y_test_non_cold_item = y_test[~mask_cold_user.values], y_test[~mask_cold_item.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:31:58.176538Z",
     "iopub.status.busy": "2025-04-27T02:31:58.176277Z",
     "iopub.status.idle": "2025-04-27T02:31:58.180801Z",
     "shell.execute_reply": "2025-04-27T02:31:58.180093Z",
     "shell.execute_reply.started": "2025-04-27T02:31:58.176519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hàm chuẩn bị input\n",
    "def prepare_input(df):\n",
    "    return [df[col].values for col in [\n",
    "        \"user_id\", \"item_id\", \"category_id\", \"parent_category\", \"timestamp_norm\", \"price\", \"age\", \"gender\",\n",
    "        \"hour\", \"dayofweek\", \"is_weekend\", \"hour_sin\", \"hour_cos\", \"dayofweek_sin\", \"dayofweek_cos\", \"time_diff\", \"session_id\"\n",
    "    ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:32:01.759455Z",
     "iopub.status.busy": "2025-04-27T02:32:01.759203Z",
     "iopub.status.idle": "2025-04-27T02:32:01.765137Z",
     "shell.execute_reply": "2025-04-27T02:32:01.764090Z",
     "shell.execute_reply.started": "2025-04-27T02:32:01.759438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hàm evaluate: thêm precision, recall, auc\n",
    "def evaluate_cold_start(model, df, y_true, name=\"Cold Start\"):\n",
    "    X_eval = prepare_input(df)\n",
    "    y_pred_probs = model.predict(X_eval, batch_size=128, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1) if len(y_true.shape) > 1 else y_true\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    auc = roc_auc_score(pd.get_dummies(y_true), y_pred_probs, multi_class='ovr') if len(np.unique(y_true)) > 1 else 0\n",
    "\n",
    "    print(f\"\\nĐánh giá cho {name}:\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:32:04.710044Z",
     "iopub.status.busy": "2025-04-27T02:32:04.709759Z",
     "iopub.status.idle": "2025-04-27T02:32:42.747881Z",
     "shell.execute_reply": "2025-04-27T02:32:42.747214Z",
     "shell.execute_reply.started": "2025-04-27T02:32:04.710006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate cho các nhóm\n",
    "evaluate_cold_start(model, test_cold_user, y_test_cold_user, name=\"User Cold Start\")\n",
    "evaluate_cold_start(model, test_cold_item, y_test_cold_item, name=\"Item Cold Start\")\n",
    "evaluate_cold_start(model, test_non_cold_user, y_test_non_cold_user, name=\"User Non-Cold Start\")\n",
    "evaluate_cold_start(model, test_non_cold_item, y_test_non_cold_item, name=\"Item Non-Cold Start\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:32:46.703836Z",
     "iopub.status.busy": "2025-04-27T02:32:46.703351Z",
     "iopub.status.idle": "2025-04-27T02:32:46.708489Z",
     "shell.execute_reply": "2025-04-27T02:32:46.707886Z",
     "shell.execute_reply.started": "2025-04-27T02:32:46.703813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hàm lấy f1-score per class\n",
    "def get_f1_per_class(model, df, y_true):\n",
    "    X_eval = prepare_input(df)\n",
    "    y_pred_probs = model.predict(X_eval, batch_size=128, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1) if len(y_true.shape) > 1 else y_true\n",
    "\n",
    "    _, _, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:32:50.518295Z",
     "iopub.status.busy": "2025-04-27T02:32:50.517738Z",
     "iopub.status.idle": "2025-04-27T02:33:20.164104Z",
     "shell.execute_reply": "2025-04-27T02:33:20.163374Z",
     "shell.execute_reply.started": "2025-04-27T02:32:50.518270Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tính f1 theo từng nhóm\n",
    "f1_user_cold = get_f1_per_class(model, test_cold_user, y_test_cold_user)\n",
    "f1_user_non_cold = get_f1_per_class(model, test_non_cold_user, y_test_non_cold_user)\n",
    "f1_item_cold = get_f1_per_class(model, test_cold_item, y_test_cold_item)\n",
    "f1_item_non_cold = get_f1_per_class(model, test_non_cold_item, y_test_non_cold_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T02:33:44.446476Z",
     "iopub.status.busy": "2025-04-27T02:33:44.445755Z",
     "iopub.status.idle": "2025-04-27T02:33:44.695916Z",
     "shell.execute_reply": "2025-04-27T02:33:44.695192Z",
     "shell.execute_reply.started": "2025-04-27T02:33:44.446451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "labels = ['Class 1', 'Class 2', 'Class 3']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - 1.5*width, f1_user_cold, width, label='User Cold-start', color='skyblue')\n",
    "plt.bar(x - 0.5*width, f1_user_non_cold, width, label='User Non-cold', color='dodgerblue')\n",
    "plt.bar(x + 0.5*width, f1_item_cold, width, label='Item Cold-start', color='lightcoral')\n",
    "plt.bar(x + 1.5*width, f1_item_non_cold, width, label='Item Non-cold', color='indianred')\n",
    "\n",
    "plt.xticks(x, labels)\n",
    "plt.xlabel(\"Class (interaction value)\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"So sánh F1-score theo class giữa Cold-start và Non-cold-start\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1031,
     "sourceId": 4471234,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7227844,
     "sourceId": 11524862,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
